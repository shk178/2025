{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f6c0a7",
   "metadata": {},
   "source": [
    "- 선형회귀분석의 가정\n",
    "    - 선형성: 종속변수와 독립변수는 선형관계\n",
    "    - 등분산성: 잔차의 분산이 고르게 분포\n",
    "    - 정상성(정규성): 잔차가 정규분포의 특성 지님\n",
    "    - 독립성: 오차가 서로 독립이며, 독립변수들간 상관관계가 없음\n",
    "    - 다중공선성: 독립변수들 간에 강한 상관관계가 나타내는 문제\n",
    "        - VIF(분산팽창인수, 1/(1-R^2)) 값이 10 이상이면 다중공선성 존재한다고 판단\n",
    "- 사례: 집값 예측\n",
    "    - 어떤 사람이 아파트 가격을 예측하려고 해요.  \n",
    "    - 독립변수(설명 변수)로 \"평수, 위치, 층수, 주차 공간 유무\"를 사용해서 종속변수인 \"집값\"을 예측하려고 합니다.\n",
    "- 선형성 (Linearity)\n",
    "    - 집값은 평수, 위치, 층수 등과 선형 관계를 가져야 해요.  \n",
    "    - 즉, 평수가 증가하면 집값도 일정한 비율로 증가해야 한다는 뜻이에요.  \n",
    "    - 하지만 선형성이 깨지는 경우, 예를 들어 너무 비싼 고급 아파트는 평수가 커져도 가격이 비슷하거나 오히려 줄어드는 경우가 생길 수 있어요.  \n",
    "    - 이럴 때는 선형회귀가 적절하지 않을 수도 있어요.\n",
    "- 등분산성 (Homoscedasticity)\n",
    "    - 잔차(예측값과 실제값의 차이)의 분산이 일정해야 해요.  \n",
    "    - 예를 들어, 작은 평수의 아파트는 예측값과 실제값이 거의 일치하는데,  \n",
    "    - 큰 평수의 아파트는 예측값과 실제값 차이가 크게 난다면 잔차의 분산이 고르지 않다고 할 수 있어요.  \n",
    "    - 이런 경우 모델이 특정 구간에서는 잘 작동하고, 다른 구간에서는 제대로 예측하지 못할 수 있어요.\n",
    "- 정상성 (Normality)\n",
    "    - 잔차(오차값)가 정규분포를 따라야 해요.  \n",
    "    - 예를 들어, 아파트 가격을 예측했을 때 오차(예측값 - 실제값)가 대부분 평균값 근처에서 고르게 분포해야 정상성 조건을 만족합니다.  \n",
    "    - 그런데 특정 가격대의 아파트에서만 유난히 큰 오차가 발생한다면, 잔차가 정규분포를 따르지 않는 거죠.\n",
    "- 독립성 (Independence)\n",
    "    - 오차는 서로 독립적이어야 해요.  \n",
    "    - 예를 들어, 같은 단지에 있는 아파트들의 가격이 비슷한데, 모델이 이를 고려하지 않고 있다면 오차가 서로 연관되어 있을 가능성이 높습니다.  \n",
    "    - 즉, 한 아파트 가격 예측이 틀리면 비슷한 아파트도 연달아 틀릴 수 있어요.  \n",
    "    - 이런 경우, 오차가 독립적이지 않다고 볼 수 있어요.\n",
    "- 다중공선성 (Multicollinearity)\n",
    "    - 독립변수들끼리 너무 강한 상관관계를 가지면 문제가 됩니다.  \n",
    "    - 예를 들어, \"평수\"와 \"방 개수\"가 독립변수로 들어갔다면?\"  \n",
    "    - 평수가 크면 방 개수도 많아지는 경향이 있기 때문에 둘 사이에 강한 상관관계가 생깁니다.  \n",
    "    - 이럴 경우 모델이 둘 중 하나의 변수만 있어도 예측이 가능할 정도로 중복된 정보를 포함하고 있다는 뜻이에요.  \n",
    "    - 이러한 다중공선성을 판단하는 지표가 VIF(분산팽창인수)인데, VIF 값이 10 이상이면 다중공선성이 심하다고 보고 한 변수는 제거하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e5aa01",
   "metadata": {},
   "source": [
    "- 회귀분석 종류\n",
    "    - 단순회귀: 1개 독립변수와 종속변수의 선형관계\n",
    "    - 다중회귀: 2개 이상 독립변수와 종속변수의 선형 관계\n",
    "    - 다항회귀: 2개 이상 독립변수와 종속변수가 2차 함수 이상의 관계\n",
    "    - 릿지회귀 (L2 규제): L2 규제항을 포함 - 시그마(W제곱) (유클리디안 거리 기반)\n",
    "    - 라쏘회귀 (L1 규제): L1 규제항을 포함 - 시그마(절댓값W) (맨하탄 거리 기반)\n",
    "    - 교호항이 포함된 회귀: 독립변수들의 교호작용이 포함된 회귀 모형\n",
    "        - 교호작용: 두 개 이상의 독립변수가 상호작용 해서 종속변수에 영향 미치는 경우\n",
    "- 최적의 회귀방정식 탐색 방법\n",
    "    - 전진선택법: 변수를 한개씩 추가하면서 찾아냄\n",
    "    - 후진제거법: 변수를 한개씩 제거하면서 찾아냄\n",
    "    - 단계별 선택법: 전진선택법+후진제거법으로 변수를 추가할 때 벌점을 고려\n",
    "        - AIC (아카이케 정보 기준): 편향과 분산이 최적화되는 지점 탐색, 자료가 많을수록 부정확\n",
    "        - BIC (베이즈 정보 기준): AIC를 보완했지만 AIC보다 큰 패널티 가지는 단점\n",
    "        - AIC, BIC 모두 작아야 좋음\n",
    "- 회귀분석의 종류\n",
    "    - 단순회귀\n",
    "        - 예를 들어, 집의 가격을 예측한다고 해보죠. 만약 오직 '집의 크기(㎡)'만을 기준으로 가격을 예측하려 한다면, 단순회귀를 사용할 수 있어요.\n",
    "        - 즉, 집이 클수록 가격이 높아지는 선형관계를 가정하는 거죠.\n",
    "    - 다중회귀  \n",
    "        - 하지만 현실은 좀 더 복잡해요. 집의 가격은 '집의 크기'뿐만 아니라 '위치', '층수', '연식' 같은 여러 요인에 영향을 받아요.\n",
    "        - 이런 경우, 여러 독립변수를 사용하는 다중회귀를 적용할 수 있어요.\n",
    "    - 다항회귀  \n",
    "        - 자동차 연비 예측을 예로 들어볼게요. 일반적으로 속도가 너무 낮거나 너무 높으면 연비가 떨어지죠.\n",
    "        - 이럴 때 단순 선형관계보다 2차 함수 이상의 관계가 필요할 수 있어요. 그래서 다항회귀를 사용해 속도와 연비의 관계를 모델링할 수 있죠.\n",
    "    - 릿지회귀 (L2 규제)\n",
    "        - 모델이 너무 복잡해지면 과적합(overfitting)이 발생할 수 있어요. 이를 방지하려면 독립변수들의 가중치(회귀 계수)를 제한하는 릿지회귀를 사용할 수 있어요.\n",
    "            - 모델 만들 때 사용한 값들에 너무 맞춰지게 그래프 그려진 걸 과적합이라고 한다.\n",
    "        - 예를 들어, 여러 요인을 사용해 주식 가격을 예측할 때 특정 요인의 영향력이 너무 커지는 걸 막아줄 수 있죠.\n",
    "    - 라쏘회귀 (L1 규제)\n",
    "        - 릿지회귀와 비슷하지만, 라쏘회귀는 일부 독립변수의 가중치를 완전히 0으로 만들어 불필요한 변수를 제거하는 특징이 있어요.\n",
    "        - 예를 들어, 다이어트 성공 여부를 예측할 때 '운동량'과 '식습관'은 중요한 변수지만, '태어난 달' 같은 변수는 영향이 없을 수 있어요.\n",
    "            - 이런 불필요한 변수를 자동으로 제거하는 데 라쏘회귀가 유용하죠.\n",
    "    - 교호항이 포함된 회귀\n",
    "        - 독립변수 간 상호작용이 중요한 경우가 있어요.\n",
    "        - 예를 들면, '커피 섭취량'과 '수면 시간'이 각각 집중력에 영향을 미칠 수 있지만, 둘이 동시에 작용하면 예상보다 더 강한 영향을 줄 수도 있어요.\n",
    "            - 커피섭취량 1, 수면시간 2 정도 영향력인데 커피섭취량*수면시간하니까 영향력이 10 이런 경우\n",
    "        - 이런 경우 교호항을 포함한 회귀 모형을 사용할 수 있어요.\n",
    "- 최적의 회귀 방정식 탐색 방법\n",
    "    - 전진선택법\n",
    "        - 예를 들어, 광고비가 매출에 미치는 영향을 분석할 때 처음에는 가장 유력한 변수부터 추가해요.\n",
    "        - 'TV 광고비'가 먼저 추가되었다가, 그다음으로 '온라인 광고비' 같은 변수가 포함될 수 있어요.\n",
    "    - 후진제거법\n",
    "        - 반대로 후진제거법은 처음엔 모든 변수를 포함한 상태에서 시작한 후, 불필요한 변수를 하나씩 제거하면서 최적의 모델을 찾는 방식이에요.\n",
    "        - 예를 들어, 매출 예측 모델에서 '사무실 청결도' 같은 변수가 큰 영향을 주지 않는다면 제거하는 식이죠.\n",
    "    - 단계별 선택법\n",
    "        - 전진선택법과 후진제거법을 결합해 변수 추가 시 벌점을 고려하는 방식이에요. 이를 위해 AIC (아카이케 정보 기준)과 BIC (베이즈 정보 기준)을 사용해 최적화된 모델을 찾죠.\n",
    "            - 예를 들어, 영화의 흥행을 예측할 때 변수를 추가하면 예측력이 올라가지만, 너무 많은 변수를 포함하면 오히려 과적합될 수 있어요.\n",
    "            - 그래서 AIC, BIC 값을 최소화하는 최적의 변수를 선택하는 과정이 필요해요.\n",
    "        - 회귀 모델을 만들 때 변수를 추가하면 예측력이 올라가는 것처럼 보이지만, 너무 많은 변수를 포함하면 과적합(overfitting)이 발생할 수 있어요.\n",
    "            - 즉, 모델이 훈련 데이터에는 잘 맞지만, 새로운 데이터에 대해서는 예측 성능이 떨어지는 문제죠.\n",
    "            - 이를 방지하기 위해 전진선택법과 후진제거법을 결합한 방식이 바로 단계별 선택법이에요.\n",
    "        - AIC와 BIC의 역할: 단계별 선택법에서는 AIC(아카이케 정보 기준)과 BIC(베이즈 정보 기준)을 이용해 최적의 모델을 찾습니다.\n",
    "            - AIC (Akaike Information Criterion)\n",
    "                - 모델의 정확성과 복잡성 사이의 균형을 맞추는 역할을 해요.\n",
    "                - 값이 작을수록 좋은 모델이라고 볼 수 있어요.\n",
    "                - 하지만 데이터의 양이 많아지면 불안정할 수 있다는 단점이 있어요.\n",
    "            - BIC (Bayesian Information Criterion)\n",
    "                - AIC와 비슷하지만, 변수 추가에 더 엄격한 벌점(penalty)을 부과해요.\n",
    "                - 즉, 불필요한 변수가 포함되는 걸 더욱 강하게 막습니다.\n",
    "                - 데이터의 크기가 커질수록 AIC보다 더 신뢰성이 높아요.\n",
    "        - 영화 흥행 예측 예시: 영화의 흥행을 예측하는 모델을 만든다고 가정해볼게요.\n",
    "            - 가능한 독립변수(예측 변수)로 다음과 같은 요소가 있을 수 있어요:\n",
    "                - ✔ 배우의 유명도 ✔ 제작비 ✔ 개봉 시즌 ✔ 감독의 경력 ✔ 마케팅 비용 ✔ 상영관 수\n",
    "            - 단계별 선택법을 적용하면, 처음엔 변수가 하나씩 추가되면서 AIC/BIC 값을 계산해요.\n",
    "            - 그러다가 특정 변수가 모델의 성능을 크게 개선하지 않거나 오히려 AIC/BIC 값을 증가시키면 그 변수를 제거하는 방식으로 최적의 모델을 찾아가는 거죠.\n",
    "            - 예를 들어, 처음에는 배우의 유명도와 제작비가 포함되었을 때 AIC/BIC 값이 감소하면 유지\n",
    "                - 하지만 감독의 경력을 추가했더니 AIC/BIC 값이 증가하면 제거\n",
    "                - 이런 과정을 반복하면서 최소한의 변수로 가장 예측력이 높은 모델을 찾아낼 수 있어요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b2b79",
   "metadata": {},
   "source": [
    "- 회귀분석의 분산분석(ANOVA Table)\n",
    "\n",
    "| 요인 | 제곱합 | 자유도 | 제곱평균 | F비 |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|회귀|SSR=시그마{(평균값\"바y\"-예측값\"ŷ\")의제곱}|p(회귀계수 수)|MSR=SSR/p||\n",
    "|잔차|SSE=시그마{(실제값\"y\"-예측값\"ŷ\")의제곱}|n(전체 파라미터 수)-p-1|MSE=SSE/(n-p-1)||\n",
    "|총|SST=SSR+SSE|n-1||F=MSR/MSE|\n",
    "\n",
    "- ANOVA 검정: 3개 이상 그룹의 평균을 비교하는 검정 (회귀모형의 유의성 분석 시 활용)\n",
    "    - 전체 데이터 수 = 자유도 + 1\n",
    "    - 결정계수(R스퀘어) = SSR/SST\n",
    "    - 수정된 R스퀘어 = 1-{(n-1)*(MSE/SST)}\n",
    "        - 다중회귀에서는 수정된 R스퀘어 값을 일반적으로 쓴다.\n",
    "- 요인(Factor)\n",
    "    - 각 행은 특정한 데이터의 분산(변동)을 설명하는 요인을 나타냅니다.  \n",
    "    - 회귀(Regression) → 모델이 데이터를 얼마나 설명하는지를 나타냄.  \n",
    "    - 잔차(Residual, Error) → 모델이 설명하지 못하는 오차.  \n",
    "    - 총(Total) → 전체 데이터의 변동량.  \n",
    "- 제곱합 (Sum of Squares, SS)\n",
    "    - 각 요인의 변동을 측정하기 위한 값이에요.\n",
    "    - SSR (회귀 제곱합)\n",
    "        - 모델이 설명할 수 있는 변동량을 의미해요.\n",
    "        - SSR = Σ(평균값 \"바y\" - 예측값 \"ŷ\")²\n",
    "    - SSE (잔차 제곱합)\n",
    "        - 모델이 설명하지 못한 오차를 의미해요.\n",
    "        - SSE = Σ(실제값 \"y\" - 예측값 \"ŷ\")²\n",
    "    - SST (총 제곱합)\n",
    "        - 전체 데이터 변동량, 즉 SSR과 SSE의 합이에요.\n",
    "        - SST = SSR + SSE  \n",
    "- 자유도 (Degrees of Freedom, df)\n",
    "    - 각 변동값을 계산할 때 얼마나 많은 정보를 사용할 수 있는지를 나타내요.\n",
    "    - p (회귀계수 수)\n",
    "        - 독립변수(설명 변수)의 개수예요.\n",
    "        - 즉, 우리가 모델을 만들 때 고려한 변수가 몇 개냐는 뜻이죠.\n",
    "    - n - p - 1 (잔차의 자유도)\n",
    "        - 전체 데이터 개수(n)에서 모델의 복잡성을 고려하여 자유도를 조정한 값이에요.\n",
    "        - 즉, 전체 데이터 중에서 우리가 추정하는 변수들을 뺀 후, 추가로 하나를 빼는 이유는 평균을 사용한 통계적 조정 때문이에요.\n",
    "    - n - 1 (총 자유도)\n",
    "        - 데이터 전체의 자유도를 의미해요.  \n",
    "- 제곱평균 (Mean Square, MS)\n",
    "    - 제곱합을 자유도로 나눈 값으로 평균적인 변동량을 나타냅니다.\n",
    "    - MSR (회귀 제곱평균)\n",
    "        - 모델이 설명하는 평균적인 변동량이에요.\n",
    "        - MSR = SSR / p\n",
    "    - MSE (잔차 제곱평균)\n",
    "        - 모델이 설명하지 못한 평균적인 오차예요.\n",
    "        - MSE = SSE / (n - p - 1)  \n",
    "- F-비 (F-Ratio)\n",
    "    - 모델의 유의성을 평가하는 값이에요.\n",
    "    - F = MSR / MSE\n",
    "        - 즉, 모델이 설명하는 변동량(MSR)이 모델이 설명하지 못한 변동량(MSE)에 비해 얼마나 큰지를 비교하는 값이에요.\n",
    "        - F-비가 클수록 모델이 의미 있다고 판단할 가능성이 커요!  \n",
    "- 정리\n",
    "    - 이 표를 통해 모델이 데이터를 얼마나 잘 설명하는지를 평가할 수 있어요!\n",
    "    - p와 n-p-1은 각각 모델의 독립변수 개수와 자유도를 조정한 값이었고,\n",
    "    - MSR과 MSE는 변동량의 평균을 의미했어요.\n",
    "    - 그리고 F-비(F=MSR/MSE)는 모델의 유의성을 검정하는 지표였죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e616b1",
   "metadata": {},
   "source": [
    "- 자유도는 표본 데이터에서 사용되는 중요한 개념이에요.\n",
    "    - 자유도의 의미\n",
    "        - 총 자유도=n-1: 표본에서 데이터를 분석할 때 실제로 독립적으로 변할 수 있는 값의 개수를 의미해요.\n",
    "            - 왜 n-1인가?: 표본에서 평균을 사용하면 하나의 값은 고정되어야 해요. 즉, 전체 n개의 데이터 중에서 마지막 하나는 평균을 유지하기 위해 종속되므로 독립적으로 변할 수 있는 값이 n-1개가 되는 거죠.\n",
    "        - 전체 데이터 개수 = 자유도 + 1\n",
    "            - 이 표현은 표본에서 전체 데이터 개수를 자유도와 관련하여 설명한 거예요.\n",
    "            - 전체 데이터 개수=n은 표본에서 데이터의 총 개수를 의미하고,\n",
    "            - 자유도 n-1는 평균을 고려하여 실제로 독립적으로 변할 수 있는 데이터 개수를 의미하는 거죠.\n",
    "    - 예제: 5명의 친구가 저녁 식사 비용을 나누는 경우\n",
    "        - 다섯 명의 친구가 함께 식사를 했고, 총 비용은 100,000원이에요. 각자 얼마나 냈는지 기록했지만, 마지막 한 명의 지불 금액을 모르고 있다고 가정해볼게요.\n",
    "        - 각 친구의 지불 금액\n",
    "            - 친구 A: 20,000원 친구 B: 18,000원 친구 C: 22,000원 친구 D: 19,000원 친구 E: ???\n",
    "        - 우리는 총액을 알고 있기 때문에, 친구 E가 얼마를 냈는지는 자동으로 결정돼요. 21,000원\n",
    "            - 총 데이터 개수 n=5 (5명의 지불 금액)\n",
    "            - 자유도 n-1=4 (독립적으로 설정할 수 있는 값의 개수)\n",
    "                - 첫 4명의 친구가 자유롭게 금액을 결정할 수 있지만, 마지막 친구 E의 금액은 자동으로 결정되기 때문에 실제로 자유롭게 변할 수 있는 개수는 ( 5-1 = 4 )개가 되는 거예요.\n",
    "        - 이런 방식으로 표본에서 자유도가 계산되는 이유를 이해할 수 있어요.\n",
    "- 결정계수 R^2와 수정된 결정계수 R^2수정값의 의미를 이해하기 쉽게 사례를 들어 설명해볼게요.  \n",
    "- 결정계수 - R^2란?\n",
    "    - 결정계수는 회귀 모델이 데이터를 얼마나 잘 설명하는지를 나타내는 값이에요.\n",
    "    - R^2 = SSR/SST\n",
    "        - SST (Total Sum of Squares): 전체 데이터의 변동성  \n",
    "        - SSR (Regression Sum of Squares): 회귀 모델이 설명하는 변동성\n",
    "    - 즉, 결정계수는 전체 데이터 변동성 중에서 모델이 설명할 수 있는 비율이에요.\n",
    "        - 값이 1에 가까울수록 모델이 데이터를 잘 설명한다는 뜻이고, 0에 가까울수록 설명력이 낮다는 의미죠.\n",
    "- 수정된 결정계수 - R^2수정값이란?\n",
    "    - R^2_수정 = 1 - {(n-1)*(MSE/SST)}\n",
    "    - 다중회귀 분석에서는 변수 개수가 많아질수록 R^2 값이 자연스럽게 증가하는 경향이 있어요.\n",
    "    - 하지만 이는 모델이 진짜로 데이터를 잘 설명해서가 아니라, 단순히 변수를 추가했기 때문일 수도 있어요.\n",
    "    - 그래서 변수 개수를 고려해 보정한 값이 수정된 결정계수예요.\n",
    "- 사례: 광고비와 매출 간의 회귀 분석 \n",
    "    - 한 회사에서 광고비와 매출의 관계를 분석하려고 해요.\n",
    "    - 모델 1 (단순회귀)\n",
    "        - R^2 = 0.75 → 광고비 하나만 사용한 모델이 전체 변동성의 75%를 설명\n",
    "    - 모델 2 (다중회귀: 광고비 + 할인율 + SNS활동 추가)\n",
    "        - R^2 = 0.82 → 변수가 추가되면서 설명력이 높아졌음\n",
    "    - 하지만 문제는, 변수를 추가한다고 해서 반드시 모델이 좋아지는 건 아니에요.\n",
    "        - 변수가 많아지면 모델이 너무 복잡해질 수도 있죠.\n",
    "    - 그래서 수정된 R^2를 보면,  \n",
    "        - 모델 1의 수정된 R^2 = 0.74\n",
    "        - 모델 2의 수정된 R^2 = 0.78\n",
    "    - 즉, 모델 2가 변수 추가로 인해 일반적인 R^2값은 높아졌지만,\n",
    "        - 수정된 R^2 값도 증가했다면 정말로 모델이 개선되었다고 볼 수 있어요.\n",
    "        - 만약 수정된 R^2 값이 작아졌다면, 불필요한 변수를 추가한 것일 수도 있어요."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
