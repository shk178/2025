{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258611c9",
   "metadata": {},
   "source": [
    "- 서포트 벡터 머신 (SVM)\n",
    "    - 마진이 최대가 되는 초평면을 찾아\n",
    "    - 선형이나 비선형 이진분류, 회귀에서 활용 가능한 다목적 모델\n",
    "    - 구성요소\n",
    "        - 하이퍼플레인(초평면)\n",
    "            - 데이터를 구분하는 기준이 되는 경계\n",
    "            - 가중치벡터와 편향으로 결정\n",
    "        - 서포트벡터\n",
    "            - 클래스를 나누는 하이퍼플레인과 가까운 위치의 샘플\n",
    "        - 마진\n",
    "            - 하이퍼플레인과 서포트벡터 사이의 거리\n",
    "                - 왼쪽에 네모 오른쪽에 세모 있으면 네모 세모 나누는 여러 직선 중에 (이런 직선이 초평면이다.)\n",
    "                - 직선-가까운 네모(가까운이 서포트벡터), 직선-가까운 세모라는 거리가 마진\n",
    "                - 마진이 제일 큰 건 \\ /기울어진 직선 아닌 |인 직선이다.\n",
    "                - 이걸 찾는 게 서포트벡터머신\n",
    "        - 커널함수\n",
    "            - 저차원 데이터를 고차원 데이터로 변경하는 함수\n",
    "                - 일직선일 때는 분류 못했는데 2차3차함수로 하니까 분류가능해짐\n",
    "    - 유형\n",
    "        - 하드마진분류: 오류 비허용\n",
    "        - 소프트마진분류: 마진내 오류 어느정도허용\n",
    "- 서포트 벡터 머신(SVM)은 데이터를 분류하는 데 사용하는 강력한 기법이에요.\n",
    "    - 쉽게 말해, 두 개의 그룹을 가장 확실하게 구분하는 선(경계)을 찾는 방법이라고 볼 수 있어요.\n",
    "    - SVM의 핵심 개념\n",
    "        - 하이퍼플레인(초평면)\n",
    "            - 데이터를 나누는 기준이 되는 선(또는 면).\n",
    "            - 예를 들어, 학생들의 성적을 기준으로 합격과 불합격을 나누는 선을 그리는 것과 비슷해요.\n",
    "        - 서포트 벡터\n",
    "            - 경계선(하이퍼플레인)과 가장 가까운 데이터 포인트.\n",
    "            - 쉽게 말해, \"우리 그룹과 저쪽 그룹 사이에서 가장 중요한 역할을 하는 데이터들\"이에요.\n",
    "        - 마진\n",
    "            - 하이퍼플레인과 서포트 벡터 사이의 거리.\n",
    "            - 마진이 넓을수록 안정적인 분류가 가능해요.\n",
    "        - 커널 함수\n",
    "            - 데이터를 더 잘 분류하기 위해 차원을 확장하는 방법.\n",
    "            - 예를 들어, 원 모양으로 분포된 데이터를 선으로 분류할 수 없을 때, 차원을 올려 분류가 가능하게 만드는 역할을 해요.\n",
    "    - SVM의 유형\n",
    "        - 하드 마진 분류\n",
    "            - \"오류를 허용하지 않고\" 완벽하게 데이터를 분리하는 방식.\n",
    "            - 예를 들어, 어떤 상품이 100% 성공할지 실패할지를 무조건 구분하려고 할 때 사용해요.\n",
    "        - 소프트 마진 분류\n",
    "            - \"약간의 오류를 허용\"하면서 최대한 올바른 분류를 찾는 방식.\n",
    "            - 현실에서는 데이터를 완벽하게 구분하는 게 어렵기 때문에, 조금 틀릴 수 있어도 최대한 좋은 경계를 찾는 게 더 중요할 때 활용해요.\n",
    "        - 쉽게 비유하면?\n",
    "            - SVM은 \"이쪽이냐 저쪽이냐?\"를 가장 정확하게 구분하는 선을 찾는 방법이에요.\n",
    "            - 예를 들어, 농구공과 축구공을 분류한다고 해볼까요?\n",
    "                - 하드 마진: \"100% 완벽한 기준\"을 적용해서 분류. (실수 허용 X)\n",
    "                - 소프트 마진: \"조금 애매한 상황도 고려하면서\" 최대한 잘 분류. (실수 허용 O)\n",
    "    - 이렇게 SVM을 활용하면 복잡한 데이터에서도 최적의 분류 기준을 찾을 수 있어요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe0612",
   "metadata": {},
   "source": [
    "- 앙상블\n",
    "    - 여러 개의 예측 모형들을 조합하는 기법으로 전체적인 분산을 감소시켜 성능 향상이 가능\n",
    "    - 보팅(Voting)\n",
    "        - 다수결 방식으로 최종 모델을 선택\n",
    "            - 모델별로 나온 결과를 투표하는 것\n",
    "    - 배깅(Bagging)\n",
    "        - 복원추출에 기반을 둔 붓스트랩을 생성해서 모델을 학습하고 이후 보팅으로 결합\n",
    "            - 어떤 데이터셋에서 복원추출로 부트스트랩1, 2, ... 여럿 생성\n",
    "            - 각각을 학습한 후에 다시 보팅한다.\n",
    "        - 복원추출 무한히 반복할 때 특정 하나의 데이터가 선택되지 않을 확률 36.8%다.\n",
    "            - 리미트N->무한대(1-(1/N))^N = 36.8%\n",
    "                - 만약 데이터셋에 데이터 N개 있고, 바탕으로 무한히 붓스트랩 생성했을 때\n",
    "                - N개 중 한 데이터가 붓스트랩으로 입력되지 않을 확률이니까\n",
    "                - (1-1/N) 붓스트랩1번에서 안 들어갈 확률.. 이 N번 곱해진다.\n",
    "            - 이 36.8%는 학습에 사용되지 않으니까, 평가에 사용할 수 있다.\n",
    "    - 부스팅(boosting)\n",
    "        - 잘못된 분류 데이터에 큰 가중치를 주는 방법 (가중치를 두고(조정하고) 다시 학습)\n",
    "        - 이상치에 민감\n",
    "        - 종류: AdaBoost, GBM, XGBoost(GBM보다 빠르고 규제 포함), Light GBM(학습속도 개선)\n",
    "    - 스태킹(Stacking)\n",
    "        - 각각의 모델에서 학습한 예측 결과를 다시 학습\n",
    "    - 랜덤포레스트\n",
    "        - 배깅에 의사결정트리를 추가하는 기법으로 성능이 좋고 이상치에 강한 모델\n",
    "    - 보팅, 배깅, 랜덤포레스트는 병렬처리 가능\n",
    "    - 부스팅은 병렬처리 불가\n",
    "- 앙상블(Ensemble) 기법은 여러 개의 모델을 결합해 더 좋은 예측 결과를 만드는 방법이에요!\n",
    "    - 하나의 모델만 사용하는 것보다 더 안정적인 결과를 얻을 수 있어요\n",
    "    - 보팅(Voting) → 다수결 방식\n",
    "        - 여러 개의 모델을 각각 실행한 후, 가장 많이 나온 결과를 최종 결정하는 방법!\n",
    "        - 예: 여러 전문가에게 같은 질문을 던지고, 가장 많이 나온 답을 정답으로 채택하는 것과 비슷해요.\n",
    "    - 배깅(Bagging) → 랜덤 샘플 추출 후 학습/여러 샘플로 나눠 각각 학습 후 종합\n",
    "        - 데이터를 여러 개의 랜덤 샘플(부트스트랩)로 나누고, 각각의 모델을 학습한 후 보팅으로 최종 결정을 내려요.\n",
    "        - 예: 학생들에게 같은 문제를 여러 번 출제하고, 각각의 시험 결과를 종합해서 성적을 평가하는 것과 같아요!\n",
    "        - 대표 알고리즘: 랜덤 포레스트(Random Forest) → 배깅 + 의사결정나무\n",
    "    - 부스팅(Boosting) → 틀린 부분을 집중 보완/틀린 부분을 점점 더 보완\n",
    "        - 처음 모델이 틀린 데이터를 강조해서 다음 모델을 더 잘 학습시키는 방식.\n",
    "        - 점점 더 정교하게 학습되지만, 이상치(예외적인 데이터)에 민감할 수도 있어요.\n",
    "            - 즉, 처음에는 단순하게 예측하지만, 반복적으로 학습하면서 틀린 부분을 보완하고 더 정확한 모델을 만들려는 과정이에요.\n",
    "            - 이상치(Outlier)는 일반적인 데이터와 다르게 특이한 값을 의미해요.\n",
    "                - 부스팅은 틀린 데이터를 더 신경 써서 학습하는 방식이에요.\n",
    "                - 그런데 만약 이상치(극단적인 데이터)가 존재한다면?\n",
    "                - 이 이상치를 너무 중요하게 여겨서 모델이 그 데이터에 지나치게 맞춰질 수도 있어요.\n",
    "            - 결국 모델이 일반적인 데이터보다는 극단적인 데이터에 영향을 많이 받게 되는 거죠.\n",
    "        - 대표 알고리즘: AdaBoost, XGBoost, LightGBM\n",
    "    - 스태킹(Stacking) → 예측 결과를 다시 학습/여러 모델의 결과를 다시 학습하는 방식!\n",
    "        - 여러 모델의 예측 결과를 조합해서 다시 학습하는 방식.\n",
    "        - 예: 여러 전문가의 답변을 취합한 후, 한 명의 최종 전문가가 분석해서 최종 결정을 내리는 느낌!\n",
    "    - 병렬 처리 가능 vs 불가능\n",
    "        - 보팅, 배깅, 랜덤 포레스트 → 각 모델을 독립적으로 학습할 수 있어 병렬처리 가능\n",
    "        - 부스팅 → 이전 모델의 결과를 바탕으로 다음 모델이 학습하기 때문에 병렬처리 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f03ca7",
   "metadata": {},
   "source": [
    "- 인공신경망\n",
    "    - 인간의 뇌 구조를 모방한 퍼셉트론을 활용한 추론모델\n",
    "    - 구조\n",
    "        - 단층신경망: 입력층, 출력층으로 구성 (단일퍼셉트론)\n",
    "        - 다층신경망: 입력층과 출력층 사이에 1개 이상의 은닉층 보유 (다층퍼셉트론)\n",
    "            - 은닉층 수는 사용자가 직접 설정하는 하이퍼파라미터\n",
    "            - (입력 0.5\\*가중치 0.6)+(입력0.3\\*가중치-2.0)+편향1.0=0.7\n",
    "            - 다중회귀구조다.\n",
    "        - 가중치는 각 퍼셉트론 간의 연결 강도를 의미\n",
    "    - 단층 신경망 (Single-layer Perceptron)\n",
    "        - 입력층과 출력층만 존재하는 간단한 구조.\n",
    "        - 각 입력 값이 가중치를 곱한 후 편향(Bias)을 더해 출력이 결정됨.\n",
    "        - 직선 형태로 데이터를 분류할 수 있음(선형 분류).\n",
    "    - 다층 신경망 (Multi-layer Perceptron, MLP)\n",
    "        - 입력층과 출력층 사이에 은닉층(hidden layer)이 추가된 구조.\n",
    "        - 복잡한 패턴을 학습할 수 있으며, 비선형 분류도 가능!\n",
    "    - 단층 신경망의 가중치는 항상 1일까?\n",
    "        - 아니요! 단층 신경망에서도 각 입력 값에 따라 가중치가 달라질 수 있어요.\n",
    "        - 모델이 학습하면서 입력 값의 중요도를 반영해 최적의 가중치를 찾는 과정을 거쳐요.\n",
    "        - 예를 들어, 단층 신경망에서 입력값이 0.5, 0.3이고, 가중치가 0.6, -2.0이라면:\n",
    "            - (0.5 × 0.6) + (0.3 × -2.0) + 편향(1.0) = 0.7\n",
    "            - 여기서 가중치가 1이 아닌 다른 값으로 조정되었죠!\n",
    "        - 가중치는 데이터에 따라 달라지는 값이며, 학습 과정을 통해 최적의 값을 찾는 중요한 요소예요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65d21b",
   "metadata": {},
   "source": [
    "- 인공신경망 이어서\n",
    "    - 활성화함수와 손실함수\n",
    "        - 은닉층에서의 활성함수: 인공신경망의 선형성을 극복 (XOR 문제 해결)\n",
    "            - 시그모이드 함수\n",
    "                - 0~1 사이의 확률 값 가지며 로지스틱 회귀와 유사\n",
    "            - 하이퍼볼릭 탄젠트(Tanh) 함수\n",
    "                - -1~1 사이 값, 시그모이드 함수의 기울기 소실을 지연시킨다.\n",
    "            - ReLU 함수\n",
    "                - 기울기 소실문제를 극복, max(0, x)\n",
    "            - 그외 활성함수\n",
    "                - Leaky RELU, GELU, ELU 등\n",
    "        - 출력층에서의 활성함수\n",
    "            - 시그모이드 함수\n",
    "                - 이진분류 모델 (0~1사이 확률)\n",
    "            - 소프트맥스 함수\n",
    "                - 다중분류 모델 (확률 총합이 1)\n",
    "        - 손실함수: 예측과 실제값의 차이를 측정하는 함수\n",
    "            - MSE (Mean Square Error) - 회귀 모델\n",
    "            - 크로스 엔트로피 (Cross-Entropy) - 분류 모델\n",
    "            - 예측값과 실제값 차이 = SSE라고 했었다..\n",
    "                - 이차함수-x축간의 거리 최소인 점 = 이차함수니까 극점인 것처럼\n",
    "                - MSE, 크로스엔트로피로 찾은 함수로 sse 최소인 점 찾을 수 있게 된다.\n",
    "    - 인공신경망 학습방법\n",
    "        - 순전파(피드포워드)\n",
    "            - 정보가 전방으로 전달\n",
    "        - 역전파 알고리즘\n",
    "            - 가중치 수정해 손실함수 값을 줄임\n",
    "            - 합성함수의 곱 활용\n",
    "        - 경사하강법\n",
    "            - 경사 내리막길로 이동해 오차가 최소가 되는 최적의 해 찾는 기법\n",
    "            - 편미분 활용\n",
    "        - 기울기 소실 문제\n",
    "            - 다수의 은닉층에서 시그모이드함수 사용시, 학습이 제대로 되지 않는 문제\n",
    "- 활성화 함수란?\n",
    "    - 뉴런이 출력값을 계산할 때 사용하는 함수예요!  \n",
    "    - 활성화 함수가 없다면 신경망이 단순한 선형 모델처럼 작동할 거라서, 복잡한 문제를 해결할 수 없어요.  \n",
    "    - 그래서 비선형성을 추가하여 더 똑똑하게 예측할 수 있도록 도와주는 역할을 합니다.\n",
    "- 은닉층 활성화 함수 (XOR 문제 해결)\n",
    "    - 입력 값을 변형해서 복잡한 패턴을 학습할 수 있도록 도와줘요.\n",
    "    - 시그모이드(Sigmoid) 함수 → 0~1 사이 값 출력  \n",
    "        - 예: \"이 이메일이 스팸일 확률은 0.8이다!\" 처럼 확률을 계산할 때 사용돼요.  \n",
    "    - Tanh (하이퍼볼릭 탄젠트) 함수 → -1~1 사이 값 출력  \n",
    "        - 시그모이드보다 더 강력하게 데이터를 변형할 수 있어요!  \n",
    "    - ReLU (Rectified Linear Unit) 함수 → 기울기 소실 문제 극복  \n",
    "        - 0 이하 값은 무시하고, 0보다 큰 값만 반영 (max(0, x))  \n",
    "        - 예: \"사진 속 고양이 특징을 더 강하게 반영할까?\" → 큰 값을 강조하는 방식  \n",
    "- 출력층 활성화 함수 (결과를 최종 결정!)\n",
    "    - 인공신경망의 최종 결과를 결정하는 함수예요.\n",
    "    - 시그모이드 → 이진 분류 (예: 스팸 vs 정상 이메일)  \n",
    "    - 소프트맥스 → 다중 분류 (예: \"이 사진은 개일까, 고양이일까, 자동차일까?\" 확률 계산)  \n",
    "- 손실 함수란?\n",
    "    - 신경망이 얼마나 틀렸는지 측정하는 함수예요!\n",
    "    - 손실 값이 크면 → \"예측이 많이 틀렸어!\" → 모델이 더 학습해야 함!  \n",
    "    - MSE (평균제곱오차, Mean Squared Error) → 회귀 문제  \n",
    "        - 예: \"내일의 기온을 예측했는데 실제 온도와 차이가 얼마나 날까?\"  \n",
    "    - 크로스 엔트로피 (Cross-Entropy) → 분류 문제  \n",
    "        - 예: \"이 사진이 개일 확률 80%, 고양이일 확률 15%, 자동차일 확률 5%인데, 실제 정답은 개였다면 얼마나 틀렸을까?\"  \n",
    "- 인공신경망 학습 방법\n",
    "    - 모델이 데이터를 보고 점점 더 똑똑해지는 과정이에요.\n",
    "    - 순전파 (Feedforward) → 정보가 앞으로만 이동  \n",
    "        - 예: \"입력 데이터를 받아서 결과를 계산하는 과정\"  \n",
    "    - 역전파 (Backpropagation) → 가중치 수정!  \n",
    "        - 예: \"예측이 틀렸다면 어디서 문제가 생겼을까? 가중치를 조정해서 다시 학습해야 해!\"  \n",
    "    - 경사 하강법 (Gradient Descent) → 최적의 가중치 찾기  \n",
    "        - 예: \"내리막길을 따라 내려가다 보면 가장 낮은 위치(최적의 값)에 도달하는 것과 같아요!\"  \n",
    "- 기울기 소실 문제란?\n",
    "    - 은닉층이 너무 많아지면 학습이 제대로 안 되는 문제예요.  \n",
    "    - 특히 시그모이드 함수를 사용하면 기울기가 너무 작아져서 제대로 학습되지 않을 수 있어요.  \n",
    "    - 그래서 ReLU 같은 새로운 활성화 함수를 사용하면 해결 가능!  \n",
    "- XOR 문제란?\n",
    "    - XOR(Exclusive OR) 연산은 입력값이 서로 다를 때만 1(True)을 출력하는 논리 연산이에요.\n",
    "    - 여기서 문제는 단순한 선형 모델(Perceptron)로는 이 패턴을 제대로 학습할 수 없다는 거예요.\n",
    "    - 왜냐하면 직선 하나만으로 1과 0을 완벽하게 구분할 수 없기 때문이에요!\n",
    "    - 왜 단층 신경망으로는 해결할 수 없을까\n",
    "        - 단층 신경망(Perceptron)은 선형 분류를 기반으로 작동해요.\n",
    "        - XOR 문제는 직선 하나로 분류할 수 없는 비선형 문제예요.\n",
    "        - 하나의 선을 그어서 데이터를 \"둘로 나누기\" 어려운 구조라서 단순한 퍼셉트론으로는 해결할 수 없어요.\n",
    "    - 해결 방법: 다층 신경망 (MLP) 활용!\n",
    "        - 은닉층(hidden layer)을 추가하면 XOR 문제 해결이 가능해요.\n",
    "        - 활성화 함수 (ReLU, Sigmoid 등)을 사용하면 모델이 더 복잡한 패턴을 학습할 수 있어요.\n",
    "        - 쉽게 말해, 더 많은 뉴런과 계층을 추가하면 XOR 문제를 해결할 수 있다!\n",
    "- SSE (Sum of Squared Errors, 합계제곱오차)\n",
    "    - 예측값과 실제값의 차이를 제곱한 후 모두 더한 값이에요.\n",
    "    - 단순히 오차의 크기를 측정하는 지표이며, 손실 함수의 기본적인 형태라고 볼 수 있어요!\n",
    "- MSE (Mean Squared Error, 평균제곱오차)\n",
    "    - SSE를 샘플 수(N)로 나눈 값. 즉, 오차의 평균을 구하는 방식이에요!\n",
    "    - SSE를 최소화하는 것과 같은 맥락이지만, 데이터를 정규화하기 위해 평균을 계산하는 과정이 추가된 것입니다.\n",
    "- 크로스 엔트로피 (Cross-Entropy)\n",
    "    - 분류 문제에서 예측값과 실제값의 차이를 측정하는 손실 함수예요.\n",
    "    - MSE처럼 \"제곱 오차\"를 사용하지 않고, 확률적인 차이를 기반으로 손실을 계산하는 방식입니다.\n",
    "- SSE 최소화와 함수 극점 찾기\n",
    "    - 손실 함수(MSE 또는 크로스 엔트로피)를 최소화하는 것은 결국 SSE 최소화와 같은 목표를 가지고 있어요.\n",
    "    - 왜냐하면, 손실 함수는 오차를 측정하는 지표이므로 손실 함수의 최소값을 찾는 과정 = 최적의 모델을 찾는 과정과 같아요.\n",
    "    - 즉, 경사 하강법(Gradient Descent)을 사용하면 손실 함수(MSE, 크로스 엔트로피)가 최소화되는 점을 찾을 수 있고, 이는 결과적으로 SSE를 줄이는 방향으로 작동해요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200217a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "757780a9",
   "metadata": {},
   "source": [
    "- 딥러닝\n",
    "    - DNN (심층신경망)\n",
    "        - 은닉층 2개이상 구성된 인공신경망\n",
    "        - 입력-은닉2개이상-출력층\n",
    "    - cnn (합성곱신경망)\n",
    "        - convolution layer와 pooling layer를 활용해 이미지 패턴 찾는 인공신경망\n",
    "        - 인풋layer-convlayer-poolinglayer-flatten-fullyconnectedlayer\n",
    "    - rnn (순환신경망)\n",
    "        - 순차적인 데이터 학습에 특화된 순환구조를 가지는 신경망\n",
    "        - 과거 정보 전달되지 않는 장기 의존성 문제 발생 가능\n",
    "            - 극복모델: lstm, gru\n",
    "    - 오토인코더\n",
    "        - 입력데이터를 인코더로 압축하고 디코더로 형태 재구성하는 비지도 학습 신경망\n",
    "        - encoder-context vector(=latent space)-decoder\n",
    "        - 생성형ai 기반모델"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
